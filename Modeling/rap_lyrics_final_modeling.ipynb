{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rap-lyric-modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8ptfRMZ0zc"
      },
      "source": [
        "# In this notebook I arrange and execute the final rap lyric model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgRhV4luZ51Z"
      },
      "source": [
        "I'll start by importing the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXsJTB6s-DM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import string, os\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category='FutureWarning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Z2I6yecCjX"
      },
      "source": [
        "Now I'll import the (mostly) cleaned csv file containing the rap lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvRdfTSitIQZ"
      },
      "source": [
        "rap_df = pd.read_csv('/content/drive/MyDrive/rap_df.csv', converters={'lyrics': eval})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "R_XiS3kYtKkC",
        "outputId": "7f4cd2a7-f9c7-4330-ce54-815764f31c9f"
      },
      "source": [
        "rap_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>song</th>\n",
              "      <th>artist</th>\n",
              "      <th>lyrics_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[Awww yeah! We in the motherfuckin' place toni...</td>\n",
              "      <td>moonstruck</td>\n",
              "      <td>actionbronson</td>\n",
              "      <td>[Intro:],Awww yeah! We in the motherfuckin' pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[\"His opponent from St. Petersburg, Florida\", ...</td>\n",
              "      <td>barryhorowitz</td>\n",
              "      <td>actionbronson</td>\n",
              "      <td>\"His opponent from St. Petersburg, Florida\",\"T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Hey yo you ready? Yeah I'm ready right, the f...</td>\n",
              "      <td>themadness</td>\n",
              "      <td>actionbronson</td>\n",
              "      <td>Hey yo you ready? Yeah I'm ready right, the fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[Bronsolino, Fuck that sitting-down rap type s...</td>\n",
              "      <td>larrycsonka</td>\n",
              "      <td>actionbronson</td>\n",
              "      <td>[Intro:],Bronsolino,Fuck that sitting-down rap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[When I'm alone, Smoking weed, sitting by the ...</td>\n",
              "      <td>ronniecoleman</td>\n",
              "      <td>actionbronson</td>\n",
              "      <td>When I'm alone,Smoking weed, sitting by the wi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                      lyrics_string\n",
              "0           0  ...  [Intro:],Awww yeah! We in the motherfuckin' pl...\n",
              "1           1  ...  \"His opponent from St. Petersburg, Florida\",\"T...\n",
              "2           2  ...  Hey yo you ready? Yeah I'm ready right, the fa...\n",
              "3           3  ...  [Intro:],Bronsolino,Fuck that sitting-down rap...\n",
              "4           4  ...  When I'm alone,Smoking weed, sitting by the wi...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-dbgTrcde0"
      },
      "source": [
        "I'll drop the extra index column, which is unnecessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "8lXpCdwBtPXf",
        "outputId": "d0b19587-f721-4d2a-eb33-ddf65c44ec46"
      },
      "source": [
        "rap_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "rap_df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lyrics</th>\n",
              "      <th>song</th>\n",
              "      <th>artist</th>\n",
              "      <th>lyrics_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Awww yeah! We in the motherfuckin' place toni...</td>\n",
              "      <td>moonstruck</td>\n",
              "      <td>actionbronson</td>\n",
              "      <td>[Intro:],Awww yeah! We in the motherfuckin' pl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              lyrics  ...                                      lyrics_string\n",
              "0  [Awww yeah! We in the motherfuckin' place toni...  ...  [Intro:],Awww yeah! We in the motherfuckin' pl...\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scqt9kNicnix"
      },
      "source": [
        "I am going to sample the lyrics because the dataset is too large for modeling. This is largely due to the massive vocabulary of the rap lyric dataset (~70,000 unique words before sampling). I'll take 40% of the existing lyrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLbogQa-yDMD"
      },
      "source": [
        "rap_df = rap_df.sample(frac=0.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP_X9KGstbF3",
        "outputId": "15b9a9cc-fc21-4889-d834-11356cb8ed13"
      },
      "source": [
        "rap_df['lyrics']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3222     [Gucci Mane's a G, G, I'm tryna sell a P, P's,...\n",
              "7684     [M.I.A. Lyrics, , \"Super Tight\", , Got my shit...\n",
              "11028    [Exclusive swave, Swavey, , Every day stuntin'...\n",
              "10561    [, Dogg Pound, Don Colion, whatever, whatever,...\n",
              "8742     [[E-Dub] Redman.. Method Man.. Lady Luck.. Def...\n",
              "                               ...                        \n",
              "1512     [Would've came back for you, I just needed tim...\n",
              "2616     ['Cause I do, 'Cause I do, 'Cause I do, Keep w...\n",
              "5409     [I told my nigga don't tell my nigga for real ...\n",
              "10334    [Yeah, yeah, yeah, yeah, come on, Yeah, my nig...\n",
              "2966     [Imma run out front to see me when you walked ...\n",
              "Name: lyrics, Length: 4832, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGm7dEebgviH"
      },
      "source": [
        "Now I'll append the lyrics to a list called all_lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "jKWJ7rvttkmV",
        "outputId": "5d6a59f1-c3e4-4be6-da60-2563a97a1b53"
      },
      "source": [
        "all_lyrics = []\n",
        "\n",
        "for i in rap_df.lyrics:\n",
        "  all_lyrics.extend(i)\n",
        "\n",
        "all_lyrics[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Gucci Mane's a G, G\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkerBusJiB8z"
      },
      "source": [
        "Now I will define the clean text function to remove punctuation and capitalisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbUHEgNMtoRO"
      },
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC0AcF_9iIKI"
      },
      "source": [
        "Now I'll remove items from the lyric list that are empty. I also noticed that there are still some lingering data quality issues, so I'll pop out lines with potentially offensive language and non lyric content again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyIKYaR0ts0l"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if i == '':\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2r8tVs_iR_W"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if 'X' in i: # X is subbed in for a potentially offensive word\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM7YOsN3iXBP"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if 'X' in i: # X is subbed in for a potentially offensive word\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5R0ztKbiX_g"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if 'X' in i: # X is subbed in for a potentially offensive word\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFC87_QYiYvL"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if ']' in i:\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efGaN5Jjie1k"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if '[' in i:\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGLET1drtx3V",
        "outputId": "af808d35-7d2a-4cae-8298-53a3f78adc3c"
      },
      "source": [
        "corpus = [clean_text(x) for x in all_lyrics]\n",
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_JKlHQqiii-"
      },
      "source": [
        "We have 292,357 lines of lyrics! Now i'll fit the word level tokenizer on the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXipdbqfu-Bc"
      },
      "source": [
        "# Note: char_level is False now\n",
        "rap_tokenizer = Tokenizer(char_level=False) \n",
        "rap_tokenizer.fit_on_texts(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3D2KM9uir4r"
      },
      "source": [
        "Now I'll save the tokenizer in order to use it later in the web application (as it is required for the text generate function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL_WiS4WvAfX"
      },
      "source": [
        "import pickle\n",
        "# saving\n",
        "with open('rap_tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(rap_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2d13ZOOi5_y"
      },
      "source": [
        "Let's check the vocabulary size of the corpus..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtVLE7_6vFdD",
        "outputId": "4ffb9908-6bd6-48f0-de4c-55ded10fb9ff"
      },
      "source": [
        "word_to_number = rap_tokenizer.word_index\n",
        "number_to_word = rap_tokenizer.index_word\n",
        "\n",
        "all_words = list(word_to_number.keys())\n",
        "\n",
        "print(f\"Vocabulary size: {len(all_words)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 47323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_5cElYi87z"
      },
      "source": [
        "47,323 words! Wow! That is why I needed to sample the data, as before there were 70,000 plus and the computation times were too long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94vcX2SJjD5v"
      },
      "source": [
        "Now I'll transform the tokenized corpus into sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZokHfmjOvJI6"
      },
      "source": [
        "dataset = rap_tokenizer.texts_to_sequences(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LmbGV7kjIN8"
      },
      "source": [
        "I'll define the sliding window length which will create the shapes for X and y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnjLRrzCvMLU",
        "outputId": "19836760-7de7-4b47-d9f5-09d1b749efa7"
      },
      "source": [
        "# sliding window\n",
        "SEQUENCE_LENGTH = 5\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for song in dataset:\n",
        "    for window_start_idx in range(len(song)-SEQUENCE_LENGTH):\n",
        "        window_end_idx = window_start_idx + SEQUENCE_LENGTH\n",
        "        X.append(song[window_start_idx: window_end_idx])\n",
        "        y.append(song[window_end_idx])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Let's look at the shapes\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(754760, 5)\n",
            "(754760,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GELZlmWrjMKG"
      },
      "source": [
        "Now I will arrange the architecture for the model. This architecture is optimized from earlier experimentation and is the same as the optimized models for the folk and pop lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koc_F84rvNwh"
      },
      "source": [
        "number_of_classes = len(all_words)+1\n",
        "\n",
        "rap_lyric_model = Sequential()\n",
        "rap_lyric_model.add(Embedding(number_of_classes, 5))\n",
        "\n",
        "\n",
        "rap_lyric_model.add(LSTM(700, activation='tanh', return_sequences=True))\n",
        "rap_lyric_model.add(BatchNormalization())\n",
        "rap_lyric_model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "rap_lyric_model.add(LSTM(350, activation='tanh', return_sequences=False))\n",
        "rap_lyric_model.add(BatchNormalization())\n",
        "rap_lyric_model.add(Dropout(0.2))\n",
        "\n",
        "rap_lyric_model.add(Dense(175, activation='relu'))\n",
        "rap_lyric_model.add(BatchNormalization())\n",
        "rap_lyric_model.add(Dropout(0.2))\n",
        "\n",
        "rap_lyric_model.add(Dense(number_of_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xZUVSxivXRG"
      },
      "source": [
        "# Compile model\n",
        "rap_lyric_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzGVQCCSvZD2",
        "outputId": "e3f4483b-e84c-4223-b287-da41f583e58b"
      },
      "source": [
        "rap_lyric_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 5)           236620    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, None, 700)         1976800   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, None, 700)         2800      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, None, 700)         0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 350)               1471400   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 350)               1400      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 350)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 175)               61425     \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 175)               700       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 175)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 47324)             8329024   \n",
            "=================================================================\n",
            "Total params: 12,080,169\n",
            "Trainable params: 12,077,719\n",
            "Non-trainable params: 2,450\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_FR7QwNjZw4"
      },
      "source": [
        "Because this model is more computationally intensive I decided to shrink the number of epochs by 50. I found I still got excellent results at this number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74uuK9bkvazu",
        "outputId": "cdeb2d95-f0ea-49ea-81e2-0858e25f6369"
      },
      "source": [
        "history = rap_lyric_model.fit(X, y,\n",
        "        batch_size=1024,\n",
        "        epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "738/738 [==============================] - 96s 125ms/step - loss: 8.0796 - accuracy: 0.0528\n",
            "Epoch 2/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 6.0582 - accuracy: 0.1011\n",
            "Epoch 3/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 5.7623 - accuracy: 0.1186\n",
            "Epoch 4/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 5.5621 - accuracy: 0.1287\n",
            "Epoch 5/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 5.3989 - accuracy: 0.1385\n",
            "Epoch 6/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 5.2687 - accuracy: 0.1456\n",
            "Epoch 7/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 5.1434 - accuracy: 0.1528\n",
            "Epoch 8/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 5.0463 - accuracy: 0.1591\n",
            "Epoch 9/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.9495 - accuracy: 0.1657\n",
            "Epoch 10/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.8697 - accuracy: 0.1706\n",
            "Epoch 11/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.7870 - accuracy: 0.1783\n",
            "Epoch 12/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.7117 - accuracy: 0.1839\n",
            "Epoch 13/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.6612 - accuracy: 0.1894\n",
            "Epoch 14/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.6065 - accuracy: 0.1942\n",
            "Epoch 15/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.5583 - accuracy: 0.1986\n",
            "Epoch 16/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.5051 - accuracy: 0.2040\n",
            "Epoch 17/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.4691 - accuracy: 0.2073\n",
            "Epoch 18/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.4285 - accuracy: 0.2118\n",
            "Epoch 19/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.3944 - accuracy: 0.2157\n",
            "Epoch 20/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.3612 - accuracy: 0.2198\n",
            "Epoch 21/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.3279 - accuracy: 0.2229\n",
            "Epoch 22/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.3052 - accuracy: 0.2257\n",
            "Epoch 23/200\n",
            "738/738 [==============================] - 92s 125ms/step - loss: 4.2745 - accuracy: 0.2294\n",
            "Epoch 24/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.2510 - accuracy: 0.2316\n",
            "Epoch 25/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.2259 - accuracy: 0.2348\n",
            "Epoch 26/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.2029 - accuracy: 0.2374\n",
            "Epoch 27/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.1845 - accuracy: 0.2392\n",
            "Epoch 28/200\n",
            "738/738 [==============================] - 93s 125ms/step - loss: 4.1614 - accuracy: 0.2416\n",
            "Epoch 29/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.1416 - accuracy: 0.2444\n",
            "Epoch 30/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.1249 - accuracy: 0.2458\n",
            "Epoch 31/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.1089 - accuracy: 0.2486\n",
            "Epoch 32/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.0891 - accuracy: 0.2501\n",
            "Epoch 33/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.0694 - accuracy: 0.2531\n",
            "Epoch 34/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.0541 - accuracy: 0.2538\n",
            "Epoch 35/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.0467 - accuracy: 0.2548\n",
            "Epoch 36/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.0312 - accuracy: 0.2566\n",
            "Epoch 37/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.0174 - accuracy: 0.2576\n",
            "Epoch 38/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 4.0021 - accuracy: 0.2608\n",
            "Epoch 39/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9874 - accuracy: 0.2629\n",
            "Epoch 40/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9802 - accuracy: 0.2634\n",
            "Epoch 41/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9650 - accuracy: 0.2646\n",
            "Epoch 42/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9564 - accuracy: 0.2661\n",
            "Epoch 43/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9417 - accuracy: 0.2666\n",
            "Epoch 44/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9337 - accuracy: 0.2681\n",
            "Epoch 45/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9117 - accuracy: 0.2703\n",
            "Epoch 46/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.9094 - accuracy: 0.2718\n",
            "Epoch 47/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8985 - accuracy: 0.2723\n",
            "Epoch 48/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8902 - accuracy: 0.2736\n",
            "Epoch 49/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8744 - accuracy: 0.2757\n",
            "Epoch 50/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8641 - accuracy: 0.2760\n",
            "Epoch 51/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8619 - accuracy: 0.2768\n",
            "Epoch 52/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8558 - accuracy: 0.2776\n",
            "Epoch 53/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8480 - accuracy: 0.2781\n",
            "Epoch 54/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8339 - accuracy: 0.2796\n",
            "Epoch 55/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8314 - accuracy: 0.2802\n",
            "Epoch 56/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8185 - accuracy: 0.2817\n",
            "Epoch 57/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8061 - accuracy: 0.2839\n",
            "Epoch 58/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.8021 - accuracy: 0.2837\n",
            "Epoch 59/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7996 - accuracy: 0.2845\n",
            "Epoch 60/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7938 - accuracy: 0.2851\n",
            "Epoch 61/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7774 - accuracy: 0.2868\n",
            "Epoch 62/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7787 - accuracy: 0.2861\n",
            "Epoch 63/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7736 - accuracy: 0.2874\n",
            "Epoch 64/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7714 - accuracy: 0.2871\n",
            "Epoch 65/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7611 - accuracy: 0.2893\n",
            "Epoch 66/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7516 - accuracy: 0.2900\n",
            "Epoch 67/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7422 - accuracy: 0.2907\n",
            "Epoch 68/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7384 - accuracy: 0.2915\n",
            "Epoch 69/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7414 - accuracy: 0.2913\n",
            "Epoch 70/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7259 - accuracy: 0.2926\n",
            "Epoch 71/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7210 - accuracy: 0.2928\n",
            "Epoch 72/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7146 - accuracy: 0.2938\n",
            "Epoch 73/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7123 - accuracy: 0.2951\n",
            "Epoch 74/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.7074 - accuracy: 0.2952\n",
            "Epoch 75/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6981 - accuracy: 0.2957\n",
            "Epoch 76/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6942 - accuracy: 0.2963\n",
            "Epoch 77/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6861 - accuracy: 0.2971\n",
            "Epoch 78/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6808 - accuracy: 0.2980\n",
            "Epoch 79/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6821 - accuracy: 0.2980\n",
            "Epoch 80/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6718 - accuracy: 0.2991\n",
            "Epoch 81/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6694 - accuracy: 0.2995\n",
            "Epoch 82/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6655 - accuracy: 0.2996\n",
            "Epoch 83/200\n",
            "738/738 [==============================] - 93s 127ms/step - loss: 3.6631 - accuracy: 0.2999\n",
            "Epoch 84/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6558 - accuracy: 0.3012\n",
            "Epoch 85/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6507 - accuracy: 0.3015\n",
            "Epoch 86/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6456 - accuracy: 0.3030\n",
            "Epoch 87/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6465 - accuracy: 0.3031\n",
            "Epoch 88/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6425 - accuracy: 0.3020\n",
            "Epoch 89/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6372 - accuracy: 0.3037\n",
            "Epoch 90/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6325 - accuracy: 0.3038\n",
            "Epoch 91/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6216 - accuracy: 0.3043\n",
            "Epoch 92/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6192 - accuracy: 0.3055\n",
            "Epoch 93/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6127 - accuracy: 0.3059\n",
            "Epoch 94/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6114 - accuracy: 0.3064\n",
            "Epoch 95/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6046 - accuracy: 0.3067\n",
            "Epoch 96/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6055 - accuracy: 0.3078\n",
            "Epoch 97/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.6031 - accuracy: 0.3076\n",
            "Epoch 98/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5984 - accuracy: 0.3075\n",
            "Epoch 99/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5891 - accuracy: 0.3084\n",
            "Epoch 100/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5861 - accuracy: 0.3101\n",
            "Epoch 101/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5826 - accuracy: 0.3099\n",
            "Epoch 102/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5817 - accuracy: 0.3099\n",
            "Epoch 103/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5755 - accuracy: 0.3100\n",
            "Epoch 104/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5776 - accuracy: 0.3114\n",
            "Epoch 105/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5723 - accuracy: 0.3116\n",
            "Epoch 106/200\n",
            "738/738 [==============================] - 93s 127ms/step - loss: 3.5660 - accuracy: 0.3121\n",
            "Epoch 107/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5625 - accuracy: 0.3115\n",
            "Epoch 108/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5702 - accuracy: 0.3126\n",
            "Epoch 109/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5537 - accuracy: 0.3128\n",
            "Epoch 110/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5510 - accuracy: 0.3146\n",
            "Epoch 111/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5483 - accuracy: 0.3141\n",
            "Epoch 112/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5448 - accuracy: 0.3147\n",
            "Epoch 113/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5378 - accuracy: 0.3149\n",
            "Epoch 114/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5368 - accuracy: 0.3156\n",
            "Epoch 115/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5378 - accuracy: 0.3150\n",
            "Epoch 116/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5297 - accuracy: 0.3164\n",
            "Epoch 117/200\n",
            "738/738 [==============================] - 93s 127ms/step - loss: 3.5225 - accuracy: 0.3190\n",
            "Epoch 118/200\n",
            "738/738 [==============================] - 93s 127ms/step - loss: 3.5226 - accuracy: 0.3173\n",
            "Epoch 119/200\n",
            "738/738 [==============================] - 93s 127ms/step - loss: 3.5241 - accuracy: 0.3170\n",
            "Epoch 120/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5144 - accuracy: 0.3190\n",
            "Epoch 121/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5125 - accuracy: 0.3184\n",
            "Epoch 122/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5083 - accuracy: 0.3191\n",
            "Epoch 123/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4997 - accuracy: 0.3198\n",
            "Epoch 124/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5011 - accuracy: 0.3202\n",
            "Epoch 125/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.5039 - accuracy: 0.3200\n",
            "Epoch 126/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4991 - accuracy: 0.3203\n",
            "Epoch 127/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4950 - accuracy: 0.3209\n",
            "Epoch 128/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4938 - accuracy: 0.3208\n",
            "Epoch 129/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4987 - accuracy: 0.3200\n",
            "Epoch 130/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4849 - accuracy: 0.3212\n",
            "Epoch 131/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4821 - accuracy: 0.3227\n",
            "Epoch 132/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4790 - accuracy: 0.3232\n",
            "Epoch 133/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4692 - accuracy: 0.3241\n",
            "Epoch 134/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4684 - accuracy: 0.3251\n",
            "Epoch 135/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4677 - accuracy: 0.3238\n",
            "Epoch 136/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4651 - accuracy: 0.3243\n",
            "Epoch 137/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4643 - accuracy: 0.3248\n",
            "Epoch 138/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4633 - accuracy: 0.3247\n",
            "Epoch 139/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4605 - accuracy: 0.3250\n",
            "Epoch 140/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4582 - accuracy: 0.3250\n",
            "Epoch 141/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4492 - accuracy: 0.3260\n",
            "Epoch 142/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4565 - accuracy: 0.3257\n",
            "Epoch 143/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4489 - accuracy: 0.3266\n",
            "Epoch 144/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4434 - accuracy: 0.3262\n",
            "Epoch 145/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4412 - accuracy: 0.3271\n",
            "Epoch 146/200\n",
            "738/738 [==============================] - 93s 127ms/step - loss: 3.4346 - accuracy: 0.3285\n",
            "Epoch 147/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4349 - accuracy: 0.3284\n",
            "Epoch 148/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4299 - accuracy: 0.3286\n",
            "Epoch 149/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4321 - accuracy: 0.3284\n",
            "Epoch 150/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4281 - accuracy: 0.3291\n",
            "Epoch 151/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4185 - accuracy: 0.3298\n",
            "Epoch 152/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4211 - accuracy: 0.3294\n",
            "Epoch 153/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4213 - accuracy: 0.3299\n",
            "Epoch 154/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4111 - accuracy: 0.3312\n",
            "Epoch 155/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4079 - accuracy: 0.3318\n",
            "Epoch 156/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4065 - accuracy: 0.3319\n",
            "Epoch 157/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4055 - accuracy: 0.3323\n",
            "Epoch 158/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4022 - accuracy: 0.3330\n",
            "Epoch 159/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.4024 - accuracy: 0.3325\n",
            "Epoch 160/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3999 - accuracy: 0.3330\n",
            "Epoch 161/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3900 - accuracy: 0.3343\n",
            "Epoch 162/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3868 - accuracy: 0.3344\n",
            "Epoch 163/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3966 - accuracy: 0.3327\n",
            "Epoch 164/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3859 - accuracy: 0.3341\n",
            "Epoch 165/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3844 - accuracy: 0.3352\n",
            "Epoch 166/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3793 - accuracy: 0.3355\n",
            "Epoch 167/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3771 - accuracy: 0.3355\n",
            "Epoch 168/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3780 - accuracy: 0.3358\n",
            "Epoch 169/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3763 - accuracy: 0.3354\n",
            "Epoch 170/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3694 - accuracy: 0.3373\n",
            "Epoch 171/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3692 - accuracy: 0.3375\n",
            "Epoch 172/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3669 - accuracy: 0.3368\n",
            "Epoch 173/200\n",
            "738/738 [==============================] - 94s 127ms/step - loss: 3.3599 - accuracy: 0.3383\n",
            "Epoch 174/200\n",
            "738/738 [==============================] - 93s 127ms/step - loss: 3.3555 - accuracy: 0.3386\n",
            "Epoch 175/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3600 - accuracy: 0.3388\n",
            "Epoch 176/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3508 - accuracy: 0.3387\n",
            "Epoch 177/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3418 - accuracy: 0.3396\n",
            "Epoch 178/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3474 - accuracy: 0.3391\n",
            "Epoch 179/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3423 - accuracy: 0.3409\n",
            "Epoch 180/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3422 - accuracy: 0.3401\n",
            "Epoch 181/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3450 - accuracy: 0.3403\n",
            "Epoch 182/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3422 - accuracy: 0.3404\n",
            "Epoch 183/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3263 - accuracy: 0.3420\n",
            "Epoch 184/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3340 - accuracy: 0.3408\n",
            "Epoch 185/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3262 - accuracy: 0.3416\n",
            "Epoch 186/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3242 - accuracy: 0.3436\n",
            "Epoch 187/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3255 - accuracy: 0.3416\n",
            "Epoch 188/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3196 - accuracy: 0.3435\n",
            "Epoch 189/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3217 - accuracy: 0.3426\n",
            "Epoch 190/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3158 - accuracy: 0.3438\n",
            "Epoch 191/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3118 - accuracy: 0.3439\n",
            "Epoch 192/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3134 - accuracy: 0.3432\n",
            "Epoch 193/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3104 - accuracy: 0.3452\n",
            "Epoch 194/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3077 - accuracy: 0.3448\n",
            "Epoch 195/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.3014 - accuracy: 0.3452\n",
            "Epoch 196/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.2955 - accuracy: 0.3464\n",
            "Epoch 197/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.2979 - accuracy: 0.3467\n",
            "Epoch 198/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.2982 - accuracy: 0.3453\n",
            "Epoch 199/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.2914 - accuracy: 0.3471\n",
            "Epoch 200/200\n",
            "738/738 [==============================] - 93s 126ms/step - loss: 3.2877 - accuracy: 0.3465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_5_t0kqvdJL"
      },
      "source": [
        "# saving the model\n",
        "rap_lyric_model.save('/content/drive/MyDrive/rap_lyric_model.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ikYT_gljiz1"
      },
      "source": [
        "I'll define the generate_text function (same function as in the other model notebooks)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2kLzIlcvhZe"
      },
      "source": [
        "def generate_text(input_phrase, next_words, model):\n",
        "    # process for the model\n",
        "    processed_phrase = rap_tokenizer.texts_to_sequences([input_phrase])[0]\n",
        "    for i in range(next_words):\n",
        "      network_input = np.array(processed_phrase[-(len(processed_phrase)):], dtype=np.float32)\n",
        "      network_input = network_input.reshape((1, (len(processed_phrase)))) \n",
        "\n",
        "      # the RNN gives the probability of each word as the next one\n",
        "      predict_proba = model.predict(network_input)[0] \n",
        "      \n",
        "      # sample one word using these chances\n",
        "      predicted_index = np.random.choice(number_of_classes, 1, p=predict_proba)[0]\n",
        "\n",
        "      # add new index at the end of our list\n",
        "      processed_phrase.append(predicted_index)\n",
        "      \n",
        "\n",
        "  # indices mapped to words - the method expects a list of lists so we need the extra bracket\n",
        "      output_phrase = rap_tokenizer.sequences_to_texts([processed_phrase])[0]\n",
        "\n",
        "    return output_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxBmN7WzjnXx"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "nr_xwueRTyOq",
        "outputId": "ec0f67db-be05-4e9d-91c3-050a4dbb7a6b"
      },
      "source": [
        "generate_text('the mountains', 10, rap_lyric_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the mountains on the block just like his mother to em in'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Wfe7tS85ckAw",
        "outputId": "62c3d50c-eec9-48d1-bfe8-342b3f1452c2"
      },
      "source": [
        "generate_text('my homie', 15, rap_lyric_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'my homie is never on the beat of the underworld in our crib pop in the sink'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "w1oUrv-ZcpA_",
        "outputId": "08004bae-d266-4ad8-cf4e-8489efb3e76d"
      },
      "source": [
        "generate_text('the wind', 15, rap_lyric_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind take a shot to the money burn it in ya wallet and you stuck slow'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "ktaRAo5qc8t9",
        "outputId": "84ff7438-f7c6-4050-9912-f1c18f86efa8"
      },
      "source": [
        "generate_text('the wind', 15, rap_lyric_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind up with your body when you do appreciate your business then i speak with the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeXwt359jo_m"
      },
      "source": [
        "Works great! There are some really interesting ideas here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jukR5Ermjr9Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}