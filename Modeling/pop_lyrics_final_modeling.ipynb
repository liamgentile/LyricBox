{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pop-lyric-modeling",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrxTt3ReUXOG"
      },
      "source": [
        "# This notebook contains the modeling process of my final pop lyric model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9cT3xviUbuy"
      },
      "source": [
        "I'll start by importing the necessary packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgD_AkXLXxr8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import string, os\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category='FutureWarning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx2MHz1YUfMx"
      },
      "source": [
        "Now I'll read in the cleaned pop lyric csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VxpqjBpX6wb"
      },
      "source": [
        "pop_df = pd.read_csv('/content/drive/MyDrive/pop_df.csv', converters={'lyrics': eval})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "TgXoCRZ-X_42",
        "outputId": "0568a072-26e8-4cb9-e72b-1f20049c654a"
      },
      "source": [
        "pop_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>song</th>\n",
              "      <th>artist</th>\n",
              "      <th>lyrics_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[He said the way my blue eyes shined, Put thos...</td>\n",
              "      <td>timmcgraw</td>\n",
              "      <td>taylorswift</td>\n",
              "      <td>He said the way my blue eyes shined,Put those ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[State the obvious, I didn't get my perfect fa...</td>\n",
              "      <td>picturetoburn</td>\n",
              "      <td>taylorswift</td>\n",
              "      <td>State the obvious, I didn't get my perfect fan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>[You have a way of coming easily to me, And wh...</td>\n",
              "      <td>coldasyou</td>\n",
              "      <td>taylorswift</td>\n",
              "      <td>You have a way of coming easily to me,And when...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>[Cory's eyes are like a jungle, He smiles, it'...</td>\n",
              "      <td>staybeautiful</td>\n",
              "      <td>taylorswift</td>\n",
              "      <td>Cory's eyes are like a jungle,He smiles, it's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>[Last Christmas I gave you my heart, But the v...</td>\n",
              "      <td>lastchristmas</td>\n",
              "      <td>taylorswift</td>\n",
              "      <td>Last Christmas I gave you my heart,But the ver...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                      lyrics_string\n",
              "0           0  ...  He said the way my blue eyes shined,Put those ...\n",
              "1           1  ...  State the obvious, I didn't get my perfect fan...\n",
              "2           4  ...  You have a way of coming easily to me,And when...\n",
              "3           7  ...  Cory's eyes are like a jungle,He smiles, it's ...\n",
              "4          11  ...  Last Christmas I gave you my heart,But the ver...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3nafALiUiRm"
      },
      "source": [
        "I'll drop the additional, unnecessary index column (Unnamed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "9Gi_jTHfYCUF",
        "outputId": "0c1c7f25-23d2-48ba-93e6-7e3c94945c05"
      },
      "source": [
        "pop_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "pop_df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lyrics</th>\n",
              "      <th>song</th>\n",
              "      <th>artist</th>\n",
              "      <th>lyrics_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[He said the way my blue eyes shined, Put thos...</td>\n",
              "      <td>timmcgraw</td>\n",
              "      <td>taylorswift</td>\n",
              "      <td>He said the way my blue eyes shined,Put those ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              lyrics  ...                                      lyrics_string\n",
              "0  [He said the way my blue eyes shined, Put thos...  ...  He said the way my blue eyes shined,Put those ...\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDuj4oPKWvgM"
      },
      "source": [
        "Let's take a look at the lyrics column, which is where we'll get the data from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucuiHGifYPXD",
        "outputId": "44a110f7-727b-48a7-c755-cae148247940"
      },
      "source": [
        "pop_df['lyrics']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [He said the way my blue eyes shined, Put thos...\n",
              "1       [State the obvious, I didn't get my perfect fa...\n",
              "2       [You have a way of coming easily to me, And wh...\n",
              "3       [Cory's eyes are like a jungle, He smiles, it'...\n",
              "4       [Last Christmas I gave you my heart, But the v...\n",
              "                              ...                        \n",
              "5698    [(I'll be home, I'll be home), , I'm dreaming ...\n",
              "5699    [Lose a layer or two and let's get lost in tim...\n",
              "5700    [Sorry I'm not so merry, But I feel like this ...\n",
              "5701    [Straight up, Tell me everything you've been t...\n",
              "5702    [I'm thinking of you, I'm thinking of you, I'm...\n",
              "Name: lyrics, Length: 5703, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D92qaKLKW1Sy"
      },
      "source": [
        "We have 5703 songs to use. Now I'll append each item in each song lyric list into a new list called all_lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "KyPih8rWYQhk",
        "outputId": "8957d33c-84b7-46ba-aae4-7fe0afc1c558"
      },
      "source": [
        "all_lyrics = []\n",
        "\n",
        "for i in pop_df.lyrics:\n",
        "  all_lyrics.extend(i)\n",
        "# taking a look at the first line in the dataset\n",
        "all_lyrics[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'He said the way my blue eyes shined'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcFyYAc6W9zw"
      },
      "source": [
        "Now I'll define a clean_text function to remove punctuation and capitalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpQUsu3hYShL"
      },
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxqohSJ6XEwX"
      },
      "source": [
        "Now I'll remove all items from the list that are blank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zykxrnFaYVFG"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if i == '':\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjlFLMTgXHzp"
      },
      "source": [
        "Using list comprehension I'll clean each item (song line) with the clean_text function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnCgFnLmYWnb",
        "outputId": "5af7abfa-88be-474b-8db3-dd6364df38d0"
      },
      "source": [
        "corpus = [clean_text(x) for x in all_lyrics]\n",
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "270654"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZGUgh5kXOjt"
      },
      "source": [
        "We have 270,654 lines of lyrics! Now I'll fit a Tokenizer on the data. I'll use word level vectorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVFw-np-YYDn"
      },
      "source": [
        "# Note: char_level is False now\n",
        "pop_tokenizer = Tokenizer(char_level=False) \n",
        "pop_tokenizer.fit_on_texts(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXgwE0QIXU4I"
      },
      "source": [
        "Now I'll save the tokenizer for later use (it is required for the generate text function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr2QKE2_ZOkh"
      },
      "source": [
        "import pickle\n",
        "# saving\n",
        "with open('pop_tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(pop_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKkQgjqUXZ10"
      },
      "source": [
        "Let's check out the vocabulary size!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkJfSXNkYj7E",
        "outputId": "666ef133-ccb9-4820-d03e-7ff3d24b4ff7"
      },
      "source": [
        "word_to_number = pop_tokenizer.word_index\n",
        "number_to_word = pop_tokenizer.index_word\n",
        "\n",
        "all_words = list(word_to_number.keys())\n",
        "\n",
        "print(f\"Vocabulary size: {len(all_words)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 26261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTBuGLB7XcM3"
      },
      "source": [
        "26,261 unique words. Nice! Now I'll transform the fitted data into sequences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPb-TJEWYoSR"
      },
      "source": [
        "dataset = pop_tokenizer.texts_to_sequences(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rvdSfKsXh4g"
      },
      "source": [
        "Now I'll define the sliding window length that will establish the shape of X and y. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwJNJb5ZYsA0",
        "outputId": "fff12b7c-a839-4acb-8ca2-fddc96b40ce6"
      },
      "source": [
        "# sliding window\n",
        "SEQUENCE_LENGTH = 5\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for song in dataset:\n",
        "    for window_start_idx in range(len(song)-SEQUENCE_LENGTH):\n",
        "        window_end_idx = window_start_idx + SEQUENCE_LENGTH\n",
        "        X.append(song[window_start_idx: window_end_idx])\n",
        "        y.append(song[window_end_idx])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Let's look at the shapes\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(518412, 5)\n",
            "(518412,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Um_4o9YYMPk"
      },
      "source": [
        "Now I'll set up the architecture of the model. This model architecture is based on experimentation from earlier models. I found having two LSTM layers and an additional hidden Dense layer to work well. The number of neurons has also been somewhat optimized through experimentation. I have applied batch normalization to normalize after each batch and dropout layers to further reduce overfitting. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksqI8dM_YsnH"
      },
      "source": [
        "number_of_classes = len(all_words)+1\n",
        "\n",
        "pop_lyric_model = Sequential()\n",
        "pop_lyric_model.add(Embedding(number_of_classes, 5))\n",
        "\n",
        "\n",
        "pop_lyric_model.add(LSTM(700, activation='tanh', return_sequences=True))\n",
        "pop_lyric_model.add(BatchNormalization())\n",
        "pop_lyric_model.add(Dropout(0.2))\n",
        "\n",
        "pop_lyric_model.add(LSTM(350, activation='tanh', return_sequences=False))\n",
        "pop_lyric_model.add(BatchNormalization())\n",
        "pop_lyric_model.add(Dropout(0.2))\n",
        "\n",
        "pop_lyric_model.add(Dense(175, activation='relu'))\n",
        "pop_lyric_model.add(BatchNormalization())\n",
        "pop_lyric_model.add(Dropout(0.2))\n",
        "\n",
        "# output layer requires activation function\n",
        "pop_lyric_model.add(Dense(number_of_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2KR1rT1Y9CY"
      },
      "source": [
        "# Compile model\n",
        "pop_lyric_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqJJ3ORfY_K5",
        "outputId": "ecf9580b-3783-41c8-aa10-9918f5e24891"
      },
      "source": [
        "pop_lyric_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 5)           131310    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 700)         1976800   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 700)         2800      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 700)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 350)               1471400   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 350)               1400      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 350)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 175)               61425     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 175)               700       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 175)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 26262)             4622112   \n",
            "=================================================================\n",
            "Total params: 8,267,947\n",
            "Trainable params: 8,265,497\n",
            "Non-trainable params: 2,450\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzkVkj5fYpWT"
      },
      "source": [
        "I found a batch size of 1024 to be a manageable amount in terms of computation for the model. Anything substantially higher was too time intensive. 250 epochs was found to be a good model length for getting a high amount of learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iObhgQKgZBvt",
        "outputId": "e307b717-e2bc-485c-d05d-48b27f36cfc3"
      },
      "source": [
        "history = pop_lyric_model.fit(X, y,\n",
        "        batch_size=1024,\n",
        "        epochs=250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "507/507 [==============================] - 37s 34ms/step - loss: 7.7736 - accuracy: 0.0599\n",
            "Epoch 2/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 5.2384 - accuracy: 0.1348\n",
            "Epoch 3/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 4.8617 - accuracy: 0.1646\n",
            "Epoch 4/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 4.5899 - accuracy: 0.1895\n",
            "Epoch 5/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 4.3848 - accuracy: 0.2090\n",
            "Epoch 6/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 4.2112 - accuracy: 0.2281\n",
            "Epoch 7/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 4.0700 - accuracy: 0.2428\n",
            "Epoch 8/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.9440 - accuracy: 0.2574\n",
            "Epoch 9/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.8399 - accuracy: 0.2700\n",
            "Epoch 10/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.7540 - accuracy: 0.2808\n",
            "Epoch 11/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.6634 - accuracy: 0.2929\n",
            "Epoch 12/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.5953 - accuracy: 0.3012\n",
            "Epoch 13/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.5235 - accuracy: 0.3123\n",
            "Epoch 14/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.4705 - accuracy: 0.3191\n",
            "Epoch 15/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.4188 - accuracy: 0.3273\n",
            "Epoch 16/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.3684 - accuracy: 0.3332\n",
            "Epoch 17/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.3303 - accuracy: 0.3381\n",
            "Epoch 18/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.2901 - accuracy: 0.3455\n",
            "Epoch 19/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.2495 - accuracy: 0.3504\n",
            "Epoch 20/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.2244 - accuracy: 0.3553\n",
            "Epoch 21/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.1759 - accuracy: 0.3613\n",
            "Epoch 22/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.1533 - accuracy: 0.3652\n",
            "Epoch 23/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.1216 - accuracy: 0.3704\n",
            "Epoch 24/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.0947 - accuracy: 0.3746\n",
            "Epoch 25/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.0710 - accuracy: 0.3778\n",
            "Epoch 26/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.0535 - accuracy: 0.3813\n",
            "Epoch 27/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 3.0221 - accuracy: 0.3853\n",
            "Epoch 28/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.9988 - accuracy: 0.3891\n",
            "Epoch 29/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.9795 - accuracy: 0.3929\n",
            "Epoch 30/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.9619 - accuracy: 0.3944\n",
            "Epoch 31/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.9387 - accuracy: 0.3987\n",
            "Epoch 32/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.9182 - accuracy: 0.4007\n",
            "Epoch 33/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.9048 - accuracy: 0.4029\n",
            "Epoch 34/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.8875 - accuracy: 0.4059\n",
            "Epoch 35/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.8734 - accuracy: 0.4092\n",
            "Epoch 36/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.8553 - accuracy: 0.4114\n",
            "Epoch 37/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.8356 - accuracy: 0.4146\n",
            "Epoch 38/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.8240 - accuracy: 0.4166\n",
            "Epoch 39/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.8142 - accuracy: 0.4174\n",
            "Epoch 40/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.8014 - accuracy: 0.4193\n",
            "Epoch 41/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7816 - accuracy: 0.4229\n",
            "Epoch 42/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7679 - accuracy: 0.4260\n",
            "Epoch 43/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7542 - accuracy: 0.4272\n",
            "Epoch 44/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7507 - accuracy: 0.4284\n",
            "Epoch 45/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7316 - accuracy: 0.4311\n",
            "Epoch 46/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7272 - accuracy: 0.4318\n",
            "Epoch 47/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7033 - accuracy: 0.4351\n",
            "Epoch 48/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.7019 - accuracy: 0.4359\n",
            "Epoch 49/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6861 - accuracy: 0.4379\n",
            "Epoch 50/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6719 - accuracy: 0.4388\n",
            "Epoch 51/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6678 - accuracy: 0.4421\n",
            "Epoch 52/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6570 - accuracy: 0.4427\n",
            "Epoch 53/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6414 - accuracy: 0.4449\n",
            "Epoch 54/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6412 - accuracy: 0.4448\n",
            "Epoch 55/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6246 - accuracy: 0.4478\n",
            "Epoch 56/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6202 - accuracy: 0.4481\n",
            "Epoch 57/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6083 - accuracy: 0.4519\n",
            "Epoch 58/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.6060 - accuracy: 0.4499\n",
            "Epoch 59/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5976 - accuracy: 0.4529\n",
            "Epoch 60/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5840 - accuracy: 0.4532\n",
            "Epoch 61/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5755 - accuracy: 0.4559\n",
            "Epoch 62/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5715 - accuracy: 0.4557\n",
            "Epoch 63/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5634 - accuracy: 0.4568\n",
            "Epoch 64/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5524 - accuracy: 0.4589\n",
            "Epoch 65/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5563 - accuracy: 0.4591\n",
            "Epoch 66/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5385 - accuracy: 0.4598\n",
            "Epoch 67/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5250 - accuracy: 0.4627\n",
            "Epoch 68/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5239 - accuracy: 0.4635\n",
            "Epoch 69/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5196 - accuracy: 0.4643\n",
            "Epoch 70/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5116 - accuracy: 0.4652\n",
            "Epoch 71/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.5009 - accuracy: 0.4675\n",
            "Epoch 72/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4985 - accuracy: 0.4685\n",
            "Epoch 73/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4857 - accuracy: 0.4689\n",
            "Epoch 74/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4775 - accuracy: 0.4720\n",
            "Epoch 75/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4766 - accuracy: 0.4710\n",
            "Epoch 76/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4685 - accuracy: 0.4718\n",
            "Epoch 77/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4631 - accuracy: 0.4729\n",
            "Epoch 78/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4595 - accuracy: 0.4729\n",
            "Epoch 79/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4514 - accuracy: 0.4744\n",
            "Epoch 80/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4489 - accuracy: 0.4754\n",
            "Epoch 81/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4310 - accuracy: 0.4791\n",
            "Epoch 82/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4294 - accuracy: 0.4789\n",
            "Epoch 83/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4360 - accuracy: 0.4784\n",
            "Epoch 84/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4201 - accuracy: 0.4797\n",
            "Epoch 85/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4170 - accuracy: 0.4812\n",
            "Epoch 86/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4098 - accuracy: 0.4817\n",
            "Epoch 87/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4047 - accuracy: 0.4822\n",
            "Epoch 88/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4036 - accuracy: 0.4822\n",
            "Epoch 89/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.4016 - accuracy: 0.4826\n",
            "Epoch 90/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3866 - accuracy: 0.4859\n",
            "Epoch 91/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3848 - accuracy: 0.4854\n",
            "Epoch 92/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3804 - accuracy: 0.4864\n",
            "Epoch 93/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3726 - accuracy: 0.4879\n",
            "Epoch 94/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3655 - accuracy: 0.4886\n",
            "Epoch 95/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3568 - accuracy: 0.4905\n",
            "Epoch 96/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3552 - accuracy: 0.4908\n",
            "Epoch 97/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3569 - accuracy: 0.4907\n",
            "Epoch 98/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3515 - accuracy: 0.4904\n",
            "Epoch 99/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3462 - accuracy: 0.4917\n",
            "Epoch 100/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3363 - accuracy: 0.4934\n",
            "Epoch 101/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3312 - accuracy: 0.4949\n",
            "Epoch 102/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3293 - accuracy: 0.4949\n",
            "Epoch 103/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3237 - accuracy: 0.4949\n",
            "Epoch 104/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3225 - accuracy: 0.4972\n",
            "Epoch 105/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3216 - accuracy: 0.4949\n",
            "Epoch 106/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3109 - accuracy: 0.4973\n",
            "Epoch 107/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3127 - accuracy: 0.4969\n",
            "Epoch 108/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2986 - accuracy: 0.5008\n",
            "Epoch 109/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.3011 - accuracy: 0.5001\n",
            "Epoch 110/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2962 - accuracy: 0.5002\n",
            "Epoch 111/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2875 - accuracy: 0.5017\n",
            "Epoch 112/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2864 - accuracy: 0.5025\n",
            "Epoch 113/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2776 - accuracy: 0.5044\n",
            "Epoch 114/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2771 - accuracy: 0.5044\n",
            "Epoch 115/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2721 - accuracy: 0.5040\n",
            "Epoch 116/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2628 - accuracy: 0.5054\n",
            "Epoch 117/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2611 - accuracy: 0.5064\n",
            "Epoch 118/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2537 - accuracy: 0.5078\n",
            "Epoch 119/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2510 - accuracy: 0.5070\n",
            "Epoch 120/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2541 - accuracy: 0.5075\n",
            "Epoch 121/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2435 - accuracy: 0.5087\n",
            "Epoch 122/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2422 - accuracy: 0.5094\n",
            "Epoch 123/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2287 - accuracy: 0.5106\n",
            "Epoch 124/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2395 - accuracy: 0.5095\n",
            "Epoch 125/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2305 - accuracy: 0.5115\n",
            "Epoch 126/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2240 - accuracy: 0.5119\n",
            "Epoch 127/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2231 - accuracy: 0.5120\n",
            "Epoch 128/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2166 - accuracy: 0.5136\n",
            "Epoch 129/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2115 - accuracy: 0.5144\n",
            "Epoch 130/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2141 - accuracy: 0.5147\n",
            "Epoch 131/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2062 - accuracy: 0.5141\n",
            "Epoch 132/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2024 - accuracy: 0.5150\n",
            "Epoch 133/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.2021 - accuracy: 0.5165\n",
            "Epoch 134/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1948 - accuracy: 0.5172\n",
            "Epoch 135/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1827 - accuracy: 0.5190\n",
            "Epoch 136/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1900 - accuracy: 0.5174\n",
            "Epoch 137/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1786 - accuracy: 0.5193\n",
            "Epoch 138/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1797 - accuracy: 0.5203\n",
            "Epoch 139/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1689 - accuracy: 0.5215\n",
            "Epoch 140/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1761 - accuracy: 0.5192\n",
            "Epoch 141/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1658 - accuracy: 0.5218\n",
            "Epoch 142/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1714 - accuracy: 0.5209\n",
            "Epoch 143/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1583 - accuracy: 0.5236\n",
            "Epoch 144/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1486 - accuracy: 0.5247\n",
            "Epoch 145/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1547 - accuracy: 0.5228\n",
            "Epoch 146/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1479 - accuracy: 0.5250\n",
            "Epoch 147/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1381 - accuracy: 0.5268\n",
            "Epoch 148/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1329 - accuracy: 0.5271\n",
            "Epoch 149/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1344 - accuracy: 0.5273\n",
            "Epoch 150/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1319 - accuracy: 0.5272\n",
            "Epoch 151/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1188 - accuracy: 0.5293\n",
            "Epoch 152/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1188 - accuracy: 0.5296\n",
            "Epoch 153/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1237 - accuracy: 0.5287\n",
            "Epoch 154/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1096 - accuracy: 0.5309\n",
            "Epoch 155/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1137 - accuracy: 0.5298\n",
            "Epoch 156/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1124 - accuracy: 0.5309\n",
            "Epoch 157/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1032 - accuracy: 0.5310\n",
            "Epoch 158/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.1040 - accuracy: 0.5326\n",
            "Epoch 159/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0994 - accuracy: 0.5323\n",
            "Epoch 160/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0932 - accuracy: 0.5331\n",
            "Epoch 161/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0916 - accuracy: 0.5338\n",
            "Epoch 162/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0782 - accuracy: 0.5355\n",
            "Epoch 163/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0741 - accuracy: 0.5374\n",
            "Epoch 164/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0725 - accuracy: 0.5388\n",
            "Epoch 165/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0751 - accuracy: 0.5361\n",
            "Epoch 166/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0680 - accuracy: 0.5384\n",
            "Epoch 167/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0697 - accuracy: 0.5383\n",
            "Epoch 168/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0612 - accuracy: 0.5390\n",
            "Epoch 169/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0610 - accuracy: 0.5393\n",
            "Epoch 170/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0546 - accuracy: 0.5409\n",
            "Epoch 171/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0513 - accuracy: 0.5410\n",
            "Epoch 172/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0502 - accuracy: 0.5417\n",
            "Epoch 173/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0353 - accuracy: 0.5430\n",
            "Epoch 174/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0450 - accuracy: 0.5421\n",
            "Epoch 175/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0366 - accuracy: 0.5431\n",
            "Epoch 176/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0314 - accuracy: 0.5435\n",
            "Epoch 177/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0271 - accuracy: 0.5452\n",
            "Epoch 178/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0297 - accuracy: 0.5442\n",
            "Epoch 179/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0235 - accuracy: 0.5456\n",
            "Epoch 180/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0222 - accuracy: 0.5453\n",
            "Epoch 181/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0162 - accuracy: 0.5469\n",
            "Epoch 182/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0148 - accuracy: 0.5477\n",
            "Epoch 183/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0125 - accuracy: 0.5474\n",
            "Epoch 184/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0055 - accuracy: 0.5482\n",
            "Epoch 185/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0097 - accuracy: 0.5482\n",
            "Epoch 186/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 2.0091 - accuracy: 0.5479\n",
            "Epoch 187/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9962 - accuracy: 0.5498\n",
            "Epoch 188/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9919 - accuracy: 0.5504\n",
            "Epoch 189/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9867 - accuracy: 0.5513\n",
            "Epoch 190/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9766 - accuracy: 0.5536\n",
            "Epoch 191/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9850 - accuracy: 0.5528\n",
            "Epoch 192/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9821 - accuracy: 0.5526\n",
            "Epoch 193/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9755 - accuracy: 0.5548\n",
            "Epoch 194/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9730 - accuracy: 0.5537\n",
            "Epoch 195/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9681 - accuracy: 0.5548\n",
            "Epoch 196/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9688 - accuracy: 0.5546\n",
            "Epoch 197/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9587 - accuracy: 0.5567\n",
            "Epoch 198/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9582 - accuracy: 0.5567\n",
            "Epoch 199/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9597 - accuracy: 0.5558\n",
            "Epoch 200/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9530 - accuracy: 0.5584\n",
            "Epoch 201/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9444 - accuracy: 0.5597\n",
            "Epoch 202/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9451 - accuracy: 0.5587\n",
            "Epoch 203/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9410 - accuracy: 0.5602\n",
            "Epoch 204/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9404 - accuracy: 0.5591\n",
            "Epoch 205/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9319 - accuracy: 0.5614\n",
            "Epoch 206/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9335 - accuracy: 0.5608\n",
            "Epoch 207/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9232 - accuracy: 0.5631\n",
            "Epoch 208/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9297 - accuracy: 0.5614\n",
            "Epoch 209/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9211 - accuracy: 0.5639\n",
            "Epoch 210/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9192 - accuracy: 0.5630\n",
            "Epoch 211/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9175 - accuracy: 0.5635\n",
            "Epoch 212/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9107 - accuracy: 0.5651\n",
            "Epoch 213/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9074 - accuracy: 0.5664\n",
            "Epoch 214/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9046 - accuracy: 0.5657\n",
            "Epoch 215/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.9054 - accuracy: 0.5659\n",
            "Epoch 216/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8950 - accuracy: 0.5672\n",
            "Epoch 217/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8953 - accuracy: 0.5681\n",
            "Epoch 218/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8895 - accuracy: 0.5686\n",
            "Epoch 219/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8834 - accuracy: 0.5691\n",
            "Epoch 220/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8847 - accuracy: 0.5686\n",
            "Epoch 221/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8828 - accuracy: 0.5695\n",
            "Epoch 222/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8758 - accuracy: 0.5708\n",
            "Epoch 223/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8771 - accuracy: 0.5710\n",
            "Epoch 224/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8701 - accuracy: 0.5721\n",
            "Epoch 225/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8683 - accuracy: 0.5720\n",
            "Epoch 226/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8632 - accuracy: 0.5732\n",
            "Epoch 227/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8641 - accuracy: 0.5734\n",
            "Epoch 228/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8556 - accuracy: 0.5749\n",
            "Epoch 229/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8534 - accuracy: 0.5750\n",
            "Epoch 230/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8571 - accuracy: 0.5749\n",
            "Epoch 231/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8522 - accuracy: 0.5756\n",
            "Epoch 232/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8440 - accuracy: 0.5768\n",
            "Epoch 233/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8410 - accuracy: 0.5759\n",
            "Epoch 234/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8428 - accuracy: 0.5766\n",
            "Epoch 235/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8427 - accuracy: 0.5762\n",
            "Epoch 236/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8322 - accuracy: 0.5782\n",
            "Epoch 237/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8372 - accuracy: 0.5774\n",
            "Epoch 238/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8347 - accuracy: 0.5778\n",
            "Epoch 239/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8247 - accuracy: 0.5797\n",
            "Epoch 240/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8213 - accuracy: 0.5814\n",
            "Epoch 241/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8241 - accuracy: 0.5798\n",
            "Epoch 242/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8186 - accuracy: 0.5808\n",
            "Epoch 243/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8179 - accuracy: 0.5808\n",
            "Epoch 244/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8188 - accuracy: 0.5805\n",
            "Epoch 245/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8111 - accuracy: 0.5818\n",
            "Epoch 246/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8067 - accuracy: 0.5833\n",
            "Epoch 247/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.7996 - accuracy: 0.5850\n",
            "Epoch 248/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.8009 - accuracy: 0.5836\n",
            "Epoch 249/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.7955 - accuracy: 0.5858\n",
            "Epoch 250/250\n",
            "507/507 [==============================] - 17s 34ms/step - loss: 1.7966 - accuracy: 0.5851\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q00xc81SZHP0"
      },
      "source": [
        "pop_lyric_model.save('/content/drive/MyDrive/pop_lyric_model.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqWxmGbRY2GM"
      },
      "source": [
        "Defining the generate_text function, which is the same as the generate_text function defined in the folk lyric model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WavepGhZLWz"
      },
      "source": [
        "def generate_text(input_phrase, next_words, model):\n",
        "    # process for the model\n",
        "    processed_phrase = pop_tokenizer.texts_to_sequences([input_phrase])[0]\n",
        "    for i in range(next_words):\n",
        "      network_input = np.array(processed_phrase[-(len(processed_phrase)):], dtype=np.float32)\n",
        "      network_input = network_input.reshape((1, (len(processed_phrase)))) \n",
        "\n",
        "      # the RNN gives the probability of each word as the next one\n",
        "      predict_proba = model.predict(network_input)[0] \n",
        "      \n",
        "      # sample one word using these chances\n",
        "      predicted_index = np.random.choice(number_of_classes, 1, p=predict_proba)[0]\n",
        "\n",
        "      # add new index at the end of our list\n",
        "      processed_phrase.append(predicted_index)\n",
        "      \n",
        "\n",
        "  # indices mapped to words - the method expects a list of lists so we need the extra bracket\n",
        "      output_phrase = pop_tokenizer.sequences_to_texts([processed_phrase])[0]\n",
        "\n",
        "    return output_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDDn8kF7Y8Cn"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "pEu0EpVQZMPm",
        "outputId": "825a8f20-de4d-4ead-dceb-44be4320a4b2"
      },
      "source": [
        "generate_text(\"the wind\", 20, pop_lyric_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind within our hearts breath was the shoulder to you do the way you feel me smiled screw all the deserves'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "01wJr9e_r2m4",
        "outputId": "c32c562a-f4ff-4783-c899-c85f963acb3f"
      },
      "source": [
        "generate_text(\"I cant believe\", 20, pop_lyric_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i cant believe it i could bite and on and on and and your kicks for you aphrodite me just caught up the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "EnwNK7PIsUT-",
        "outputId": "2b72f6fb-9439-46b9-9bde-16fee6a7a2cb"
      },
      "source": [
        "generate_text(\"shes so wonderful\", 20, pop_lyric_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'shes so wonderful so deep to not talk than the stars budge n go and cried to the united it love me now'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqIvYhEnZLaS"
      },
      "source": [
        "Looks good! These are some interesting outputs."
      ]
    }
  ]
}