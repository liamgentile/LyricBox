{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "folk-lyrics-modeling-4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6p7qq5mPieV"
      },
      "source": [
        "# This notebook contains the final folk-lyric model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0YU823OPsl6"
      },
      "source": [
        "First, I will import the necessary packages..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy3n-zdwwPqD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization, GRU\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import string, os\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category='FutureWarning')\n",
        "import keras.models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJDthurnPyBg"
      },
      "source": [
        "Now, I'll import my cleaned folk lyric csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6b92JD4wVRO"
      },
      "source": [
        "folk_df = pd.read_csv('/content/drive/MyDrive/folk_df.csv', converters={'lyrics': eval})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "d1zyyMBEwYQV",
        "outputId": "36b28490-4592-4382-d14a-9fbefc61b232"
      },
      "source": [
        "folk_df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>lyrics</th>\n",
              "      <th>song</th>\n",
              "      <th>artist</th>\n",
              "      <th>lyrics_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[Living on free food tickets. Water in the mil...</td>\n",
              "      <td>theloveofcommonpeople</td>\n",
              "      <td>johndenver</td>\n",
              "      <td>Living on free food tickets. Water in the milk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[Do you remember days not so very long ago, wh...</td>\n",
              "      <td>catchanotherbutterfly</td>\n",
              "      <td>johndenver</td>\n",
              "      <td>Do you remember days not so very long ago, whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[Yes, I been dreaming about you every day, eac...</td>\n",
              "      <td>daydream</td>\n",
              "      <td>johndenver</td>\n",
              "      <td>Yes, I been dreaming about you every day, each...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[I'll sing you a song of Spiro Agnew and all t...</td>\n",
              "      <td>theballadofspiroagnew</td>\n",
              "      <td>johndenver</td>\n",
              "      <td>I'll sing you a song of Spiro Agnew and all th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[If I look like home to you, if I am your sign...</td>\n",
              "      <td>circus</td>\n",
              "      <td>johndenver</td>\n",
              "      <td>If I look like home to you, if I am your sign,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                      lyrics_string\n",
              "0           0  ...  Living on free food tickets. Water in the milk...\n",
              "1           1  ...  Do you remember days not so very long ago, whe...\n",
              "2           2  ...  Yes, I been dreaming about you every day, each...\n",
              "3           3  ...  I'll sing you a song of Spiro Agnew and all th...\n",
              "4           4  ...  If I look like home to you, if I am your sign,...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vx8YeXvP2vG"
      },
      "source": [
        "I'll quickly drop the extra index column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "tXVV6P4wweaE",
        "outputId": "78d342eb-ddb5-4aed-87c3-9255b984ff9d"
      },
      "source": [
        "folk_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "folk_df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lyrics</th>\n",
              "      <th>song</th>\n",
              "      <th>artist</th>\n",
              "      <th>lyrics_string</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Living on free food tickets. Water in the mil...</td>\n",
              "      <td>theloveofcommonpeople</td>\n",
              "      <td>johndenver</td>\n",
              "      <td>Living on free food tickets. Water in the milk...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              lyrics  ...                                      lyrics_string\n",
              "0  [Living on free food tickets. Water in the mil...  ...  Living on free food tickets. Water in the milk...\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xzBHlkrP9FZ"
      },
      "source": [
        "Let's take a look at the lyrics column, which is what I'll be using for the modeling. Each song is a list and each item in the list is a line in the song."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1e9ei1Dwfts",
        "outputId": "64458072-35a7-4048-ebce-a5595dc43be6"
      },
      "source": [
        "folk_df['lyrics']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [Living on free food tickets. Water in the mil...\n",
              "1       [Do you remember days not so very long ago, wh...\n",
              "2       [Yes, I been dreaming about you every day, eac...\n",
              "3       [I'll sing you a song of Spiro Agnew and all t...\n",
              "4       [If I look like home to you, if I am your sign...\n",
              "                              ...                        \n",
              "6720    [Our hearts are free, So tell me what's wrong ...\n",
              "6721    [It's less hard than it should be to find a de...\n",
              "6722    [I found you once, and I'll find you again, Yo...\n",
              "6723    [Flyin' on past in your voodoo mask, High on y...\n",
              "6724    [We met in a parking lot, I was buying coffee ...\n",
              "Name: lyrics, Length: 6725, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdKH2e7RQJlX"
      },
      "source": [
        "We have 6724 songs to use. Now I'll define an empty list and append these lyrics into it, line by line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "1vSpcpfdwheE",
        "outputId": "ebe15509-c40d-4f7c-eb90-b401f9167752"
      },
      "source": [
        "all_lyrics = []\n",
        "\n",
        "for i in folk_df.lyrics:\n",
        "  all_lyrics.extend(i)\n",
        "\n",
        "# checking out the first line\n",
        "all_lyrics[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Living on free food tickets. Water in the milk from the hole in the roof'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrRhlocfQVVM"
      },
      "source": [
        "Now I'll write a simple function to remove punctuation and make the text lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTOO5ZjiwjEz"
      },
      "source": [
        "def clean_text(txt):\n",
        "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
        "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return txt  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbF4X6GnQbM6"
      },
      "source": [
        "Now i'll loop through all_lyrics and remove any blank items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c3Oc0vswkhj"
      },
      "source": [
        "for i in all_lyrics:\n",
        "  if i == '':\n",
        "    all_lyrics.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWlghiUPQhsR"
      },
      "source": [
        "I'll use list comprehension to clean the lyrics with the clean_text function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LxARW27wl71",
        "outputId": "25c98c61-8dab-4487-9673-f377a7e105bb"
      },
      "source": [
        "corpus = [clean_text(x) for x in all_lyrics]\n",
        "# checking how many lines we have\n",
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205781"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfGcu1koQrDg"
      },
      "source": [
        "Great! we have 205,781 lines of lyrics. Now I'll use the Tokenizer class to vectorize the lyrics. I'll use word level vectorization/tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5jEYKwNwpWY"
      },
      "source": [
        "# setting up the class and fitting it on the corpus\n",
        "folk_tokenizer = Tokenizer(char_level=False) \n",
        "folk_tokenizer.fit_on_texts(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hrFcgW_Q5xb"
      },
      "source": [
        "Now I'll save the tokenizer so that I can use it in a web app later on (as it's required for the text generate function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV9L57pSUTwJ"
      },
      "source": [
        "import pickle\n",
        "# saving\n",
        "with open('folk_tokenizer.pkl', 'wb') as handle:\n",
        "    pickle.dump(folk_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLzNcNpQRAYS"
      },
      "source": [
        "Let's check out the vocabulary size of the model..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prkih9lzwqqq",
        "outputId": "3707d407-1f86-4351-ef42-fa17bf47b8cc"
      },
      "source": [
        "word_to_number = folk_tokenizer.word_index\n",
        "number_to_word = folk_tokenizer.index_word\n",
        "\n",
        "all_words = list(word_to_number.keys())\n",
        "\n",
        "print(f\"Vocabulary size: {len(all_words)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 29825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDoNjzEcRFWi"
      },
      "source": [
        "Wow! We have 29,825 unique words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJr9z4nlRJnu"
      },
      "source": [
        "Now I'll transform the corpus that has been fit on the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cct0J9u-wsHB"
      },
      "source": [
        "dataset = folk_tokenizer.texts_to_sequences(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d94_kuwYRPEf"
      },
      "source": [
        "I'll define the sliding window that will define X and y. I found that a sequence length of 5 led to good results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmVK8725wu-6",
        "outputId": "fe6bf6d7-8d99-4ae9-a054-44723ac16891"
      },
      "source": [
        "# sliding window\n",
        "SEQUENCE_LENGTH = 5\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for song in dataset:\n",
        "    for window_start_idx in range(len(song)-SEQUENCE_LENGTH):\n",
        "        window_end_idx = window_start_idx + SEQUENCE_LENGTH\n",
        "        X.append(song[window_start_idx: window_end_idx])\n",
        "        y.append(song[window_end_idx])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Let's look at the shapes\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(423164, 5)\n",
            "(423164,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16s84MeARYkO"
      },
      "source": [
        "Now I will set up the architecture of the model. This model architecture is based on a number of modifications from earlier model iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxlVvxF4w_c4"
      },
      "source": [
        "number_of_classes = len(all_words)+1\n",
        "\n",
        "RNN_folk_lyrics_4 = Sequential()\n",
        "RNN_folk_lyrics_4.add(Embedding(number_of_classes, 5))\n",
        "\n",
        "# the intermediate recurrent layers should return full sequences\n",
        "RNN_folk_lyrics_4.add(LSTM(700, activation='tanh', return_sequences=True))\n",
        "RNN_folk_lyrics_4.add(BatchNormalization())\n",
        "# reducing overfitting\n",
        "RNN_folk_lyrics_4.add(Dropout(0.2))\n",
        "\n",
        "# a second LSTM layer that does not return the sequences\n",
        "RNN_folk_lyrics_4.add(LSTM(350, activation='tanh', return_sequences=False))\n",
        "RNN_folk_lyrics_4.add(BatchNormalization())\n",
        "# reducing overfitting\n",
        "RNN_folk_lyrics_4.add(Dropout(0.2))\n",
        "\n",
        "# an additional dense layer to narrow down the number of neurons towards the last layer\n",
        "RNN_folk_lyrics_4.add(Dense(175, activation='relu'))\n",
        "RNN_folk_lyrics_4.add(BatchNormalization())\n",
        "#reducing overfitting\n",
        "RNN_folk_lyrics_4.add(Dropout(0.2))\n",
        "\n",
        "# the output layer requires an activation function\n",
        "RNN_folk_lyrics_4.add(Dense(number_of_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1956uvoGxLUP"
      },
      "source": [
        "# Compile model\n",
        "RNN_folk_lyrics_4.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZr2eBCUxQzP",
        "outputId": "68806ff9-59ef-4148-e876-3ec96a26e8c9"
      },
      "source": [
        "RNN_folk_lyrics_4.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 5)           149130    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, None, 700)         1976800   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 700)         2800      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 700)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 350)               1471400   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 350)               1400      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 350)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 175)               61425     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 175)               700       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 175)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 29826)             5249376   \n",
            "=================================================================\n",
            "Total params: 8,913,031\n",
            "Trainable params: 8,910,581\n",
            "Non-trainable params: 2,450\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT6pFQ4QR-KM"
      },
      "source": [
        "I will use a batch size of 1024 to make the dataset more computationally manageable. I found 250 epochs to be a good amount of epochs for learning. Beyond that there did not seem to be much improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlCQwhn5xSlM",
        "outputId": "4f8732c9-4a93-4687-aa1f-c9211fb4a37f"
      },
      "source": [
        "history = RNN_folk_lyrics_4.fit(X, y,\n",
        "        batch_size=1024,\n",
        "        epochs=250)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "414/414 [==============================] - 57s 95ms/step - loss: 8.3095 - accuracy: 0.0553\n",
            "Epoch 2/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 5.7353 - accuracy: 0.1117\n",
            "Epoch 3/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 5.3802 - accuracy: 0.1325\n",
            "Epoch 4/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 5.1132 - accuracy: 0.1505\n",
            "Epoch 5/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 4.8921 - accuracy: 0.1667\n",
            "Epoch 6/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 4.7126 - accuracy: 0.1805\n",
            "Epoch 7/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 4.5532 - accuracy: 0.1936\n",
            "Epoch 8/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 4.4170 - accuracy: 0.2059\n",
            "Epoch 9/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 4.2981 - accuracy: 0.2172\n",
            "Epoch 10/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 4.1850 - accuracy: 0.2299\n",
            "Epoch 11/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 4.0945 - accuracy: 0.2395\n",
            "Epoch 12/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 4.0041 - accuracy: 0.2497\n",
            "Epoch 13/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.9229 - accuracy: 0.2599\n",
            "Epoch 14/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.8596 - accuracy: 0.2666\n",
            "Epoch 15/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.7991 - accuracy: 0.2754\n",
            "Epoch 16/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.7484 - accuracy: 0.2815\n",
            "Epoch 17/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.6918 - accuracy: 0.2895\n",
            "Epoch 18/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.6411 - accuracy: 0.2953\n",
            "Epoch 19/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.6049 - accuracy: 0.2990\n",
            "Epoch 20/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.5534 - accuracy: 0.3060\n",
            "Epoch 21/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.5252 - accuracy: 0.3113\n",
            "Epoch 22/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.4830 - accuracy: 0.3163\n",
            "Epoch 23/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.4584 - accuracy: 0.3185\n",
            "Epoch 24/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.4213 - accuracy: 0.3248\n",
            "Epoch 25/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.3891 - accuracy: 0.3284\n",
            "Epoch 26/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.3582 - accuracy: 0.3329\n",
            "Epoch 27/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.3399 - accuracy: 0.3359\n",
            "Epoch 28/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.3041 - accuracy: 0.3401\n",
            "Epoch 29/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.2790 - accuracy: 0.3445\n",
            "Epoch 30/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.2611 - accuracy: 0.3451\n",
            "Epoch 31/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.2280 - accuracy: 0.3508\n",
            "Epoch 32/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.2137 - accuracy: 0.3517\n",
            "Epoch 33/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.1981 - accuracy: 0.3549\n",
            "Epoch 34/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.1777 - accuracy: 0.3575\n",
            "Epoch 35/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.1541 - accuracy: 0.3611\n",
            "Epoch 36/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.1300 - accuracy: 0.3637\n",
            "Epoch 37/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.1152 - accuracy: 0.3677\n",
            "Epoch 38/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.0976 - accuracy: 0.3689\n",
            "Epoch 39/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.0735 - accuracy: 0.3727\n",
            "Epoch 40/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.0617 - accuracy: 0.3751\n",
            "Epoch 41/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.0490 - accuracy: 0.3754\n",
            "Epoch 42/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.0374 - accuracy: 0.3765\n",
            "Epoch 43/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.0171 - accuracy: 0.3802\n",
            "Epoch 44/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 3.0040 - accuracy: 0.3822\n",
            "Epoch 45/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9920 - accuracy: 0.3844\n",
            "Epoch 46/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9754 - accuracy: 0.3860\n",
            "Epoch 47/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9687 - accuracy: 0.3879\n",
            "Epoch 48/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9559 - accuracy: 0.3894\n",
            "Epoch 49/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9342 - accuracy: 0.3921\n",
            "Epoch 50/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9297 - accuracy: 0.3936\n",
            "Epoch 51/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9128 - accuracy: 0.3962\n",
            "Epoch 52/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.9034 - accuracy: 0.3973\n",
            "Epoch 53/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.8974 - accuracy: 0.3969\n",
            "Epoch 54/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8787 - accuracy: 0.4007\n",
            "Epoch 55/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8719 - accuracy: 0.4011\n",
            "Epoch 56/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8596 - accuracy: 0.4027\n",
            "Epoch 57/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8508 - accuracy: 0.4038\n",
            "Epoch 58/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8441 - accuracy: 0.4049\n",
            "Epoch 59/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8317 - accuracy: 0.4069\n",
            "Epoch 60/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8201 - accuracy: 0.4067\n",
            "Epoch 61/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7993 - accuracy: 0.4115\n",
            "Epoch 62/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7980 - accuracy: 0.4129\n",
            "Epoch 63/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.8045 - accuracy: 0.4106\n",
            "Epoch 64/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7789 - accuracy: 0.4160\n",
            "Epoch 65/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7774 - accuracy: 0.4145\n",
            "Epoch 66/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7708 - accuracy: 0.4156\n",
            "Epoch 67/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7583 - accuracy: 0.4173\n",
            "Epoch 68/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7432 - accuracy: 0.4198\n",
            "Epoch 69/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7319 - accuracy: 0.4225\n",
            "Epoch 70/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7341 - accuracy: 0.4212\n",
            "Epoch 71/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7191 - accuracy: 0.4240\n",
            "Epoch 72/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7091 - accuracy: 0.4235\n",
            "Epoch 73/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.7118 - accuracy: 0.4248\n",
            "Epoch 74/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6992 - accuracy: 0.4268\n",
            "Epoch 75/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6993 - accuracy: 0.4271\n",
            "Epoch 76/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6877 - accuracy: 0.4285\n",
            "Epoch 77/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6767 - accuracy: 0.4296\n",
            "Epoch 78/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6694 - accuracy: 0.4308\n",
            "Epoch 79/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6679 - accuracy: 0.4309\n",
            "Epoch 80/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6654 - accuracy: 0.4320\n",
            "Epoch 81/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6526 - accuracy: 0.4335\n",
            "Epoch 82/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6456 - accuracy: 0.4342\n",
            "Epoch 83/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6458 - accuracy: 0.4353\n",
            "Epoch 84/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6323 - accuracy: 0.4364\n",
            "Epoch 85/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6249 - accuracy: 0.4377\n",
            "Epoch 86/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6201 - accuracy: 0.4383\n",
            "Epoch 87/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6237 - accuracy: 0.4367\n",
            "Epoch 88/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6090 - accuracy: 0.4401\n",
            "Epoch 89/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.6047 - accuracy: 0.4409\n",
            "Epoch 90/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5944 - accuracy: 0.4427\n",
            "Epoch 91/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5924 - accuracy: 0.4437\n",
            "Epoch 92/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5827 - accuracy: 0.4441\n",
            "Epoch 93/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5827 - accuracy: 0.4448\n",
            "Epoch 94/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5806 - accuracy: 0.4448\n",
            "Epoch 95/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5694 - accuracy: 0.4469\n",
            "Epoch 96/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5669 - accuracy: 0.4467\n",
            "Epoch 97/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5610 - accuracy: 0.4476\n",
            "Epoch 98/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5473 - accuracy: 0.4496\n",
            "Epoch 99/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5548 - accuracy: 0.4470\n",
            "Epoch 100/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5415 - accuracy: 0.4505\n",
            "Epoch 101/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5374 - accuracy: 0.4503\n",
            "Epoch 102/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5256 - accuracy: 0.4520\n",
            "Epoch 103/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5257 - accuracy: 0.4539\n",
            "Epoch 104/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5171 - accuracy: 0.4542\n",
            "Epoch 105/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5181 - accuracy: 0.4536\n",
            "Epoch 106/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5142 - accuracy: 0.4555\n",
            "Epoch 107/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5024 - accuracy: 0.4568\n",
            "Epoch 108/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.5001 - accuracy: 0.4563\n",
            "Epoch 109/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4931 - accuracy: 0.4574\n",
            "Epoch 110/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4914 - accuracy: 0.4579\n",
            "Epoch 111/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4870 - accuracy: 0.4587\n",
            "Epoch 112/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4800 - accuracy: 0.4595\n",
            "Epoch 113/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4829 - accuracy: 0.4600\n",
            "Epoch 114/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4733 - accuracy: 0.4609\n",
            "Epoch 115/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4685 - accuracy: 0.4614\n",
            "Epoch 116/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4606 - accuracy: 0.4622\n",
            "Epoch 117/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4574 - accuracy: 0.4642\n",
            "Epoch 118/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4598 - accuracy: 0.4629\n",
            "Epoch 119/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4462 - accuracy: 0.4655\n",
            "Epoch 120/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4443 - accuracy: 0.4651\n",
            "Epoch 121/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4473 - accuracy: 0.4631\n",
            "Epoch 122/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4432 - accuracy: 0.4655\n",
            "Epoch 123/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4432 - accuracy: 0.4651\n",
            "Epoch 124/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4283 - accuracy: 0.4680\n",
            "Epoch 125/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4231 - accuracy: 0.4689\n",
            "Epoch 126/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4204 - accuracy: 0.4686\n",
            "Epoch 127/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4060 - accuracy: 0.4712\n",
            "Epoch 128/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4176 - accuracy: 0.4700\n",
            "Epoch 129/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4119 - accuracy: 0.4700\n",
            "Epoch 130/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4059 - accuracy: 0.4711\n",
            "Epoch 131/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.4022 - accuracy: 0.4728\n",
            "Epoch 132/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3959 - accuracy: 0.4724\n",
            "Epoch 133/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3969 - accuracy: 0.4735\n",
            "Epoch 134/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3841 - accuracy: 0.4748\n",
            "Epoch 135/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3833 - accuracy: 0.4748\n",
            "Epoch 136/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3796 - accuracy: 0.4750\n",
            "Epoch 137/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3752 - accuracy: 0.4757\n",
            "Epoch 138/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3754 - accuracy: 0.4760\n",
            "Epoch 139/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3666 - accuracy: 0.4779\n",
            "Epoch 140/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3622 - accuracy: 0.4775\n",
            "Epoch 141/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3552 - accuracy: 0.4788\n",
            "Epoch 142/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3567 - accuracy: 0.4794\n",
            "Epoch 143/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3609 - accuracy: 0.4782\n",
            "Epoch 144/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3541 - accuracy: 0.4787\n",
            "Epoch 145/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3454 - accuracy: 0.4820\n",
            "Epoch 146/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3433 - accuracy: 0.4810\n",
            "Epoch 147/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3471 - accuracy: 0.4804\n",
            "Epoch 148/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3379 - accuracy: 0.4833\n",
            "Epoch 149/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3368 - accuracy: 0.4828\n",
            "Epoch 150/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3257 - accuracy: 0.4846\n",
            "Epoch 151/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3279 - accuracy: 0.4823\n",
            "Epoch 152/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3199 - accuracy: 0.4856\n",
            "Epoch 153/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3221 - accuracy: 0.4834\n",
            "Epoch 154/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3117 - accuracy: 0.4867\n",
            "Epoch 155/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3123 - accuracy: 0.4857\n",
            "Epoch 156/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3172 - accuracy: 0.4855\n",
            "Epoch 157/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3066 - accuracy: 0.4872\n",
            "Epoch 158/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.3046 - accuracy: 0.4873\n",
            "Epoch 159/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2971 - accuracy: 0.4878\n",
            "Epoch 160/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2999 - accuracy: 0.4884\n",
            "Epoch 161/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2808 - accuracy: 0.4913\n",
            "Epoch 162/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2843 - accuracy: 0.4913\n",
            "Epoch 163/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2806 - accuracy: 0.4896\n",
            "Epoch 164/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2730 - accuracy: 0.4926\n",
            "Epoch 165/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2848 - accuracy: 0.4891\n",
            "Epoch 166/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2701 - accuracy: 0.4929\n",
            "Epoch 167/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2794 - accuracy: 0.4903\n",
            "Epoch 168/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2583 - accuracy: 0.4940\n",
            "Epoch 169/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2561 - accuracy: 0.4955\n",
            "Epoch 170/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2555 - accuracy: 0.4958\n",
            "Epoch 171/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2568 - accuracy: 0.4952\n",
            "Epoch 172/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2510 - accuracy: 0.4946\n",
            "Epoch 173/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2456 - accuracy: 0.4962\n",
            "Epoch 174/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2431 - accuracy: 0.4974\n",
            "Epoch 175/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2441 - accuracy: 0.4974\n",
            "Epoch 176/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2447 - accuracy: 0.4967\n",
            "Epoch 177/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2364 - accuracy: 0.4975\n",
            "Epoch 178/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2297 - accuracy: 0.5006\n",
            "Epoch 179/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2339 - accuracy: 0.4980\n",
            "Epoch 180/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2230 - accuracy: 0.5008\n",
            "Epoch 181/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2190 - accuracy: 0.5010\n",
            "Epoch 182/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2212 - accuracy: 0.5009\n",
            "Epoch 183/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2186 - accuracy: 0.5017\n",
            "Epoch 184/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2009 - accuracy: 0.5046\n",
            "Epoch 185/250\n",
            "414/414 [==============================] - 39s 94ms/step - loss: 2.2095 - accuracy: 0.5031\n",
            "Epoch 186/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2063 - accuracy: 0.5043\n",
            "Epoch 187/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2071 - accuracy: 0.5041\n",
            "Epoch 188/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.2003 - accuracy: 0.5026\n",
            "Epoch 189/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1987 - accuracy: 0.5040\n",
            "Epoch 190/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1931 - accuracy: 0.5050\n",
            "Epoch 191/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1914 - accuracy: 0.5055\n",
            "Epoch 192/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1888 - accuracy: 0.5065\n",
            "Epoch 193/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1813 - accuracy: 0.5063\n",
            "Epoch 194/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1822 - accuracy: 0.5069\n",
            "Epoch 195/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1743 - accuracy: 0.5079\n",
            "Epoch 196/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1784 - accuracy: 0.5079\n",
            "Epoch 197/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1728 - accuracy: 0.5086\n",
            "Epoch 198/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1609 - accuracy: 0.5115\n",
            "Epoch 199/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.1657 - accuracy: 0.5100\n",
            "Epoch 200/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.1506 - accuracy: 0.5126\n",
            "Epoch 201/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1525 - accuracy: 0.5123\n",
            "Epoch 202/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1572 - accuracy: 0.5117\n",
            "Epoch 203/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1525 - accuracy: 0.5111\n",
            "Epoch 204/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1452 - accuracy: 0.5142\n",
            "Epoch 205/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1397 - accuracy: 0.5145\n",
            "Epoch 206/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1413 - accuracy: 0.5139\n",
            "Epoch 207/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1261 - accuracy: 0.5182\n",
            "Epoch 208/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1374 - accuracy: 0.5147\n",
            "Epoch 209/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1312 - accuracy: 0.5169\n",
            "Epoch 210/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1249 - accuracy: 0.5168\n",
            "Epoch 211/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1272 - accuracy: 0.5157\n",
            "Epoch 212/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1178 - accuracy: 0.5175\n",
            "Epoch 213/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1161 - accuracy: 0.5178\n",
            "Epoch 214/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1198 - accuracy: 0.5173\n",
            "Epoch 215/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1162 - accuracy: 0.5179\n",
            "Epoch 216/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1101 - accuracy: 0.5186\n",
            "Epoch 217/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1011 - accuracy: 0.5211\n",
            "Epoch 218/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1067 - accuracy: 0.5197\n",
            "Epoch 219/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0935 - accuracy: 0.5234\n",
            "Epoch 220/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0978 - accuracy: 0.5218\n",
            "Epoch 221/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.1006 - accuracy: 0.5207\n",
            "Epoch 222/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0895 - accuracy: 0.5231\n",
            "Epoch 223/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0889 - accuracy: 0.5220\n",
            "Epoch 224/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0805 - accuracy: 0.5242\n",
            "Epoch 225/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0747 - accuracy: 0.5248\n",
            "Epoch 226/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0788 - accuracy: 0.5235\n",
            "Epoch 227/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0763 - accuracy: 0.5237\n",
            "Epoch 228/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0700 - accuracy: 0.5257\n",
            "Epoch 229/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0729 - accuracy: 0.5259\n",
            "Epoch 230/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0699 - accuracy: 0.5251\n",
            "Epoch 231/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0651 - accuracy: 0.5267\n",
            "Epoch 232/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0557 - accuracy: 0.5285\n",
            "Epoch 233/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0611 - accuracy: 0.5276\n",
            "Epoch 234/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0585 - accuracy: 0.5269\n",
            "Epoch 235/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0439 - accuracy: 0.5297\n",
            "Epoch 236/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0496 - accuracy: 0.5293\n",
            "Epoch 237/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0315 - accuracy: 0.5318\n",
            "Epoch 238/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0365 - accuracy: 0.5320\n",
            "Epoch 239/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0382 - accuracy: 0.5310\n",
            "Epoch 240/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0356 - accuracy: 0.5319\n",
            "Epoch 241/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0280 - accuracy: 0.5318\n",
            "Epoch 242/250\n",
            "414/414 [==============================] - 40s 97ms/step - loss: 2.0290 - accuracy: 0.5333\n",
            "Epoch 243/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0235 - accuracy: 0.5342\n",
            "Epoch 244/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0137 - accuracy: 0.5358\n",
            "Epoch 245/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0206 - accuracy: 0.5325\n",
            "Epoch 246/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0216 - accuracy: 0.5331\n",
            "Epoch 247/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0094 - accuracy: 0.5346\n",
            "Epoch 248/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0075 - accuracy: 0.5355\n",
            "Epoch 249/250\n",
            "414/414 [==============================] - 40s 96ms/step - loss: 2.0157 - accuracy: 0.5345\n",
            "Epoch 250/250\n",
            "414/414 [==============================] - 39s 95ms/step - loss: 2.0024 - accuracy: 0.5370\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO13a_EESNJm"
      },
      "source": [
        "Saving the model for later use..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFZJPva3xvQ4"
      },
      "source": [
        "RNN_folk_lyrics_4.save('/content/drive/MyDrive/folk_lyrics_RNN_model4.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anr0x6dBSQiT"
      },
      "source": [
        "Now I'll define the generate text function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVnsNW2IxxNY"
      },
      "source": [
        "def generate_text(input_phrase, next_words, model):\n",
        "    # process for the model\n",
        "    processed_phrase = tokenizer.texts_to_sequences([input_phrase])[0]\n",
        "    for i in range(next_words):\n",
        "      network_input = np.array(processed_phrase[-(len(processed_phrase)):], dtype=np.float32)\n",
        "      network_input = network_input.reshape((1, (len(processed_phrase)))) \n",
        "\n",
        "      # the RNN gives the probability of each word as the next one\n",
        "      predict_proba = model.predict(network_input)[0] \n",
        "      \n",
        "      # sample one word using these chances\n",
        "      predicted_index = np.random.choice(number_of_classes, 1, p=predict_proba)[0]\n",
        "\n",
        "      # add new index at the end of our list\n",
        "      processed_phrase.append(predicted_index)\n",
        "      \n",
        "\n",
        "  # indices mapped to words - the method expects a list of lists so we need the extra bracket\n",
        "      output_phrase = tokenizer.sequences_to_texts([processed_phrase])[0]\n",
        "\n",
        "    return output_phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C02PIVjSZqW"
      },
      "source": [
        "Let's run some tests!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "D82hHqURYhdm",
        "outputId": "415882cd-b199-43fa-ece0-d9eb9e2c06f0"
      },
      "source": [
        "generate_text(\"the wind\", 10, RNN_folk_lyrics_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind is blowin up the wind is rain by the light'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "E0Y0o9yEYnOT",
        "outputId": "f179ba0d-eabc-4980-8858-914933352125"
      },
      "source": [
        "generate_text(\"the wind\", 10, RNN_folk_lyrics_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind go down and the train is me in a night'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Qs1J9qzcYpYX",
        "outputId": "653b6d9d-4e10-4745-ccd7-ac0e72c4fd2f"
      },
      "source": [
        "generate_text(\"the wind\", 20, RNN_folk_lyrics_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind was down the paper has you seen the world of mortals and married here and im not busy gonna stay'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "M7rYrp83Y3o0",
        "outputId": "1cb05379-ec9a-4fb9-858a-1ef37ba675b8"
      },
      "source": [
        "generate_text(\"the wind\", 25, RNN_folk_lyrics_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind had falling and such i killed him to stray from your tangles baby i just dont know the reasons to be there anyway me can'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "eeeKM7HfZAay",
        "outputId": "f2826832-daa9-4f19-bd30-4d2586a13735"
      },
      "source": [
        "generate_text(\"the mountains\", 25, RNN_folk_lyrics_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the mountains is turning on the skies no hidden on me its a sin to me more than youre the only sorry to you i will never'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "VghckCL-ZHPS",
        "outputId": "0e6290c3-548d-4391-8760-0a81a3f75cdf"
      },
      "source": [
        "generate_text(\"the wind\", 10, RNN_folk_lyrics_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the wind is blowin away today the fresh worm asked your face'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Z-ygsWlzZOif",
        "outputId": "585574b7-803d-44fa-e410-88567cc7f01c"
      },
      "source": [
        "generate_text(\"the mountains\", 10, RNN_folk_lyrics_4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the mountains are cold and the cops and the dimes and the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JicQ4B16SeI1"
      },
      "source": [
        "There is some inspiring stuff in there! Not perfect of course but it achieves my goal of generating interesting ideas. "
      ]
    }
  ]
}